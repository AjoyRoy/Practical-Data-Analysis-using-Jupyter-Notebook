{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Preparing to work with unstructured data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import the following libraries by adding the following command in your Jupyter Notebook and run the cell"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import nltk",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, we download the specific corpus we want to use. Alternatively you can download all the packages using the all parameter. If you are behind a firewall, there is an nltk.set_proxy option available. Check the documentation on http://www.nltk.org/ for more details:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('brown')",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package brown to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To confirm the package is available in your Jupyter Notebook, we can use the following command to reference the corpus using a common alias of brown for reference:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import brown",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To display a list of the few words available in the Brown Corpos, use the following command:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "brown.words()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To count all the words available we can use the len() function which counts the length of a string or the number of items in an object like an array of values. Since our values are separated by commas, it will count all the words available. To make it easier to format, let’s assign the output to a variable called count_of_words which we can use in the next step:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "count_of_words = len(brown.words())",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To make the output easier to understand to the consumer of this data, we use the print() and format() functions to display the results using the following command:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print('Count of all the words found the Brown Corpus =',format(count_of_words,',d'))",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Count of all the words found the Brown Corpus = 1,161,192\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## What is Tokenization and why is important"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import the following libraries by adding the following command in your Jupyter Notebook and run the cell. Feel free to follow along by creating your own Notebook and I have placed a copy in GitHub for reference:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('punkt')",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package punkt to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we will create a new variable called input_sentence and assign it to a free form text sentence which must be encapsulated in double quotes. There will be no input after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_sentence = \"Seth and Becca love to run down to the playground when the weather is nice.\"",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next we will use the word_tokenize() function that is available in the NLTK library to break up the individual words and any punctuation:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.word_tokenize(input_sentence)",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "['Seth',\n 'and',\n 'Becca',\n 'love',\n 'to',\n 'run',\n 'down',\n 'to',\n 'the',\n 'playground',\n 'when',\n 'the',\n 'weather',\n 'is',\n 'nice',\n '.']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next lets tokenize by sentence which requires you to import the sent_tokenize option from the NLTK tokenize library using the following command:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let’s assign a new variable called input_data to a collection of sentences which we can use later in our code. There will be no input after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data = \"Seth and Becca love the playground.  When it sunny, they head down there to play.\"",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Then we will pass the variable input_data as a parameter to the sent_tokenize() function which will look at the string of text and break them out as individual token values. We wrap the output with the print() function to display the results cleaner in the Notebook:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(sent_tokenize(input_data))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Seth and Becca love the playground.', 'When it sunny, they head down there to play.']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\ntokenized_word=nltk.word_tokenize(input_sentence)\nprint(tokenized_word)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Seth', 'and', 'Becca', 'love', 'to', 'run', 'down', 'to', 'the', 'playground', 'when', 'the', 'weather', 'is', 'nice', '.']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Counting words and exploring results"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import the probability module available in the NTLK library to count the frequency of the words available in a body of text. There will be no result returned after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.probability import FreqDist",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next explore a large body of text using the Brown Corpus. To do this, we assign the population of all the token words available by using the FreqDist() function and assigning it to a variable named input_data. To see the results of the processing of this data, we can print the variable:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data = FreqDist(brown.words())\nprint(input_data)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<FreqDist with 56057 samples and 1161192 outcomes>\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To see a list of the most common words that exist in our input_data, we can use the most_common() function along with a parameter to control how many are displayed. In this case, we want to see the top 10:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data.most_common(10)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "[('the', 62713),\n (',', 58334),\n ('.', 49346),\n ('of', 36080),\n ('and', 27915),\n ('to', 25732),\n ('a', 21881),\n ('in', 19536),\n ('that', 10237),\n ('is', 10011)]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Normalizing text techniques"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Import the PorterStemmer module available in the NTLK library to normalize a word. There will be no result returned after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.stem import PorterStemmer",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To import an instance of this feature so it can be referenced later in the code, we use the following code. There will be no result returned after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_word_stemmer = PorterStemmer()",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you can pass individual words into the instance to see how the word would be normalized:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_word_stemmer.stem('fishing')",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "'fish'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To use the lemmas features, we need to download the WordNet corpus using the following command:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('wordnet')",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package wordnet to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To import an instance of this feature so it can be referenced later in the code, we use the following code. There will be no result returned after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.stem import WordNetLemmatizer\nmy_word_lemmatizer = WordNetLemmatizer()",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To see how the lemma would output for the same word we used a stem for earlier, we pass the same word into the lemmatize() function:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_word_lemmatizer.lemmatize('fishing')",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "'fishing'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To create a list but limit the words to only a sample by assigning it to a variable, we use the following command. There will be no result returned after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "my_list_of_words= brown.words()[:10]",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we create a loop against each value in the list and print the results. We include some formatting to make it easier to understand the results for each row:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "for x in my_list_of_words:\n    print('word =', x, ': stem =', my_word_stemmer.stem(x), ': lemma =', my_word_lemmatizer.lemmatize(x))",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": "word = The : stem = The : lemma = The\nword = Fulton : stem = Fulton : lemma = Fulton\nword = County : stem = Counti : lemma = County\nword = Grand : stem = Grand : lemma = Grand\nword = Jury : stem = Juri : lemma = Jury\nword = said : stem = said : lemma = said\nword = Friday : stem = Friday : lemma = Friday\nword = an : stem = an : lemma = an\nword = investigation : stem = investig : lemma = investigation\nword = of : stem = of : lemma = of\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Excluding words from analysis"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Download the stopwords corpus from the NLTK library using the following command:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nltk.download('stopwords')",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[nltk_data] Downloading package stopwords to /home/nbuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, import the stopwords and word_tokenize features so they can be used later in the exercise:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let’s assign a new variable called input_data to a collection of sentences which we can use later in our code. There will be no input after you run the cell:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data = \"Seth and Becca love the playground.  When it sunny, they head down there to play.\"",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will assign object variables called stop_words and word_tokens so they can be referenced later in the code:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "stop_words = set(stopwords.words('english'))\nword_tokens = word_tokenize(input_data)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally we have a few lines of code that will loop through the word tokens from the input_data and compare them to the stop_words. If they match, they will be excluded. The final result prints the original input_data which has been tokenized along with the results after the stopwords have been removed. Be sure to use the correct indentation when entering the code:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "input_data_cleaned = [x for x in word_tokens if not x in stop_words]\ninput_data_cleaned = []\n\nfor x in word_tokens:\n    if x not in stop_words:\n        input_data_cleaned.append(x)       \n\nprint(word_tokens)\nprint(input_data_cleaned)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Seth', 'and', 'Becca', 'love', 'the', 'playground', '.', 'When', 'it', 'sunny', ',', 'they', 'head', 'down', 'there', 'to', 'play', '.']\n['Seth', 'Becca', 'love', 'playground', '.', 'When', 'sunny', ',', 'head', 'play', '.']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}